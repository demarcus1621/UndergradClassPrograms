structural hazards
data hazards
control hazards
	delayed branch slot
	predict the branch & do conditional execution:
	capability to undo all consequences of executing
	wrong instruction
		predict the branch as always taken
			super simple
			not that accurate
		store 1 bit of info about recently executed branches
			1 = was taken last time
			0 = was not taken last time
			predict it will do same thing as last time
			update the bit when you know whether it was taken
		store 2 bits of info about each recent branch
		
	for i = 1 to 10
		LD R1, #1
		LOOP |
			 |
			ADD R1,R1,#1
			SUB R2,R1,#11
			BNZ R2, LOOP
			
Memory:
static RAM - fast (< 1 ns), but big circuit. Used for onboard CPU storage
or registers
dynamic RAM - not quite as fast (worst case 10 - 15 ns), small circuit,
must be periodically refreshed
Both sRAM and dRAM lose state without power
flash - Electrical Erasable Programmable Read Only Memory, much slower, retains data
without power, "wears out"

Speed & Latency
< 1 ns registers (SRAM)
    cache
~10 ns main memory (DRAM)
	SSD (flash)
1 ms HDD (spinning magnetic disk)
5 ms+ cloud (network)

Cache Memory:
Locality
	temporal
	spatial
cache - content addressable memory between CPU and main memory
memory - 1-D array of bytes, where each byte has an address
block - contiguous group of addresses, where size is always a power of 2

4 Questions that guide cache design:
	1. Where in the cache can a block of addresses be stored
		a. anywhere (fully associative)
		b. one specific place (direct mapped)
		c. some places (set associative)
	2. How do you find a block
	3. What to do on a miss
		goal: minimize cache misses
		Approximate teh optimal strategy
		optimal: replace the block in cache that wont be used for the longest
		time into the future
			random - fast, but need random numbers
			FIFO - simple, fast (shift register)
			Least Recently Used (LRU) - LRU block gets thrown out, have to keep
			time stamp + have way to keep oldest time stamp (in hardware)
	4. How to handle writes
		Hit:
			i. change value in cache (write-back), inconsistent values in cache
			and main memory
			must write block back to main memory when it is flushed from cache
			ii. change value in both memory and cache (write-through), no need
			to write any block back when flushed
			main memory and cache are consistent
		Write Miss:
			i. load block into cache (write allocate)
			ii. don't load block into cache 
			
Example:
	24 bit address
	256 byte block
	32 Kbyte cache size
	
256 byte block size => offset = 2**8 
	32K / 256 = 2**(15 - 8) = 2**7 
Suppose cache is fully associative => 16 bit tag #


Average memory access time eq:
accessTime = cache_hit_time + miss_rate * miss_penalty
	miss_penalty = access time for main memory
	
Which is better? 32K unified cache or 16K I-cache/16K D-cache
hit time = 1 cycle 
miss penalty = 200 cycles(typo in book)
36% of instructions access data in memory
misses per 1000 instructions (16K I-cache) - 3.82
misses per 1000 instructions (16K D-cache) - 40.9
misses per 1000 instructions (32K unified) - 43.3
hit time for unified cache takes 1 extra cycle for load/store instruction

first calculate miss rates per data access
16K I-cache:
	3.82 / 1000 
	______________    = 0.004
		1.00 <- data access / instruction
		
16K D-cache
	40.9 / 1000
	______________    = 0.114
		0.36

32K unified
	43.3 / 1000
	____________      = 0.318
		1.36 
		
0.36 / (1 + 0.36) = 26% of data access that are load/store
^ 74% are instruction fetch

split I/D - cache 
0.74 * (1 + 0.004 * 200) + 0.26 * (1 + 0.114 * 200)
	= 7.52 cycles
	
unified
0.74 * (1 + 0.318 * 200) + 0.26 *(1 + 1 + 0.318 * 200)
	= 7.62 cycles
	
	
How much does the cache help?
200 cycles miss penalty
1.0 CPI w/o stalls
1.5 memory references/instruction
30 misses / 1000 instructions

with cache:
	execution_time = instruction_count * cycle_time * cycles_per_instruction
	CPI = 1.0 + (30 / 1000) * 200
	execution_time = 7.0 * CT * IC
	
w/o cache
	CPI = (1 + 1.5 * 200) = 301
	execution_time = 301 * IC * CT
	
speedup = 301 / 7.0 <= significant speedup

Cache Optimization:
	reduce hit time
	reduce miss rate
		sources of misses
			first time -> can't fix
			capacity miss i.e it's full
				make cache bigger
			conflict misses
				increase/decrease associativity
	reduce miss penalty
	increase block size
	virtual memory
	multi layer cache
	